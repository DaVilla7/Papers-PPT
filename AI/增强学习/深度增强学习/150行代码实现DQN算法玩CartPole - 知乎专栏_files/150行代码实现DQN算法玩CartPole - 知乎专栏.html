<!doctype html>
<html lang="zh-CN" class="">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>150行代码实现DQN算法玩CartPole - 知乎专栏</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
    <link rel="shortcut icon" href="https://static.zhihu.com/static/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="//static.zhihu.com/hemingway/app.13112b7271488a41e2ad6a89f8a0c329.css" />
    <style></style>
    <script>document.documentElement.className += ('ontouchstart' in window) ? ' touch' : ' no-touch'</script>
  </head>
  <body>

    <div id="react-root"></div>
    <textarea id="clientConfig" hidden>{"debug":false,"apiRoot":"","paySDK":"https://pay.zhihu.com/api/js","wechatConfigAPI":"/api/wechat/jssdkconfig","name":"production","instance":"column","tokens":{"X-XSRF-TOKEN":"2|9f07807a|fc34e34da636e442b23ee34faa2ab449f934ad43f937b457aa65b54caf62e54fa737b34e|1500341544","X-UDID":"\"AIDCh1qVEwyPTrgVVET4TLdzawWMU8spTaE=|1500252650\"","Authorization":["Mi4wQUFBQUhSVXFBQUFBZ01LSFdwVVREQmNBQUFCaEFsVk5CNWVUV1FDSW0xdFZSQXhUMnJJbFh3UXpwYVFJLXJFQ19n","1500252679","e590b705e5f8c5115158281477b6c1f72ad2418b"]}}</textarea>
    <textarea id="preloadedState" hidden>{"database":{"Post":{"21477488":{"title":"150行代码实现DQN算法玩CartPole","author":"flood-sung","content":"<h2>1 前言</h2><p>终于到了DQN系列真正的实战了。今天我们将一步一步的告诉大家如何用最短的代码实现基本的DQN算法，并且完成基本的RL任务。这恐怕也将是你在网上能找到的最详尽的DQN实战教程，当然了，代码也会是最短的。</p><p>在本次实战中，我们不选择Atari游戏，而使用OpenAI Gym中的传统增强学习任务之一CartPole作为练手的任务。之所以不选择Atari游戏，有两点原因：一个是训练Atari要很久，一个是Atari的一些图像的处理需要更多的tricks。而CartPole任务则比较简单。</p><p><noscript><img src=\"https://pic1.zhimg.com/7f5efad55c77e25ddd6eac786ff62b34_b.png\" data-rawwidth=\"2350\" data-rawheight=\"996\" class=\"origin_image zh-lightbox-thumb\" width=\"2350\" data-original=\"https://pic1.zhimg.com/7f5efad55c77e25ddd6eac786ff62b34_r.png\">上图就是</noscript><img src=\"//zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg\" data-rawwidth=\"2350\" data-rawheight=\"996\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2350\" data-original=\"https://pic1.zhimg.com/7f5efad55c77e25ddd6eac786ff62b34_r.png\" data-actualsrc=\"https://pic1.zhimg.com/7f5efad55c77e25ddd6eac786ff62b34_b.png\">上图就是<a href=\"http://link.zhihu.com/?target=https%3A//gym.openai.com/envs/CartPole-v0\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CartPole的基本任务示意图<i class=\"icon-external\"></i></a>，基本要求就是控制下面的cart移动使连接在上面的杆保持垂直不倒。这个任务简化到只有两个离散动作，要么向左用力，要么向右用力。而state状态就是这个杆的位置和速度。</p><p>今天我们就要用DQN来解决这个问题。</p><h2>2 完成前提</h2><p>虽然之前的文章已经说了很多，但是为了完成这个练习，大家还是需要一定的基础的：</p><ul><li>熟悉Python编程，能够使用Python基本的语法</li><li>对Tensorflow有一定的了解，知道基本的使用</li><li>知道如何使用OpenAI Gym</li><li>了解基本的神经网络MLP</li><li>理解DQN算法</li></ul><p>看起来似乎是蛮有难度，但是如果你一步一步看过来的话，这些前提都很容易满足。</p><h2>3 开始</h2><p>先上一下最后的测试效果图：</p><p><noscript><img src=\"https://pic1.zhimg.com/5893833825e464cb4ba0d5fea0fda308_b.png\" data-rawwidth=\"2396\" data-rawheight=\"984\" class=\"origin_image zh-lightbox-thumb\" width=\"2396\" data-original=\"https://pic1.zhimg.com/5893833825e464cb4ba0d5fea0fda308_r.png\">也就是100%解决问题！</noscript><img src=\"//zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg\" data-rawwidth=\"2396\" data-rawheight=\"984\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2396\" data-original=\"https://pic1.zhimg.com/5893833825e464cb4ba0d5fea0fda308_r.png\" data-actualsrc=\"https://pic1.zhimg.com/5893833825e464cb4ba0d5fea0fda308_b.png\">也就是100%解决问题！</p><p>链接：<a href=\"http://link.zhihu.com/?target=https%3A//gym.openai.com/evaluations/eval_kBouPnRtQCezgE79s6aA5A\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">gym.openai.com/evaluati</span><span class=\"invisible\">ons/eval_kBouPnRtQCezgE79s6aA5A</span><span class=\"ellipsis\"></span><i class=\"icon-external\"></i></a></p><p>我们将要实现的是最基本的DQN，也就是NIPS 13版本的DQN：</p><p><noscript><img src=\"https://pic1.zhimg.com/5473510ac001e787c034580656e49b8c_b.png\" data-rawwidth=\"1304\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb\" width=\"1304\" data-original=\"https://pic1.zhimg.com/5473510ac001e787c034580656e49b8c_r.png\">面对CartPole问题，我们进一步简化：</noscript><img src=\"//zhstatic.zhihu.com/assets/zhihu/ztext/whitedot.jpg\" data-rawwidth=\"1304\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1304\" data-original=\"https://pic1.zhimg.com/5473510ac001e787c034580656e49b8c_r.png\" data-actualsrc=\"https://pic1.zhimg.com/5473510ac001e787c034580656e49b8c_b.png\">面对CartPole问题，我们进一步简化：</p><ol><li>无需预处理Preprocessing。也就是直接获取观察Observation作为状态state输入。</li><li>只使用最基本的MLP神经网络，而不使用卷积神经网络。</li></ol><h2>3.1 编写主程序</h2><p>按照至上而下的编程方式，我们先写主函数用来执行这个实验，然后再具体编写DQN算法实现。</p><p>先import所需的库：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>import gym\nimport tensorflow as tf \nimport numpy as np \nimport random\nfrom collections import deque\n</code></pre></div><p>编写主函数如下：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span># Hyper Parameters\nENV_NAME = 'CartPole-v0'\nEPISODE = 10000 # Episode limitation\nSTEP = 300 # Step limitation in an episode\n\ndef main():\n  # initialize OpenAI Gym env and dqn agent\n  env = gym.make(ENV_NAME)\n  agent = DQN(env)\n\n  for episode in xrange(EPISODE):\n    # initialize task\n    state = env.reset()\n    # Train\n    for step in xrange(STEP):\n      action = agent.egreedy_action(state) # e-greedy action for train\n      next_state,reward,done,_ = env.step(action)\n      # Define reward for agent\n      reward_agent = -1 if done else 0.1\n      agent.perceive(state,action,reward,next_state,done)\n      state = next_state\n      if done:\n        break\n\nif __name__ == '__main__':\n  main()\n</code></pre></div><p>我们将编写一个DQN的类，DQN的一切都将封装在里面。在主函数中，我们只需调用</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>agent.egreedy_action(state) # 获取包含随机的动作\nagent.perceive(state,action,reward,next_state,done) # 感知信息\n</code></pre></div><p>本质上就是一个输出动作，一个输入状态。当然我们这里输入的是整个transition。</p><p>然后环境自己执行动作，输出新的状态：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>next_state,reward,done,_ = env.step(action)\n</code></pre></div><p>然后整个过程就反复循环，一个episode结束，就再来一个。</p><p>这就是训练的过程。</p><p>但只有训练显然不够，我们还需要测试。因此，在main()的最后，我们再加上几行的测试代码：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span># Test every 100 episodes\n    if episode % 100 == 0:\n      total_reward = 0\n      for i in xrange(TEST):\n        state = env.reset()\n        for j in xrange(STEP):\n          env.render()\n          action = agent.action(state) # direct action for test\n          state,reward,done,_ = env.step(action)\n          total_reward += reward\n          if done:\n            break\n      ave_reward = total_reward/TEST\n      print 'episode: ',episode,'Evaluation Average Reward:',ave_reward\n      if ave_reward &amp;gt;= 200:\n        break\n</code></pre></div><p>测试中唯一的不同就是我们使用</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>action = agent.action(state)\n</code></pre></div><p>来获取动作，也就是完全没有随机性，只根据神经网络来输出，没有探索，同时这里也就不再perceive输入信息来训练。</p><p>OK，这就是基本的主函数。接下来就是实现DQN</p><h2>3.2 DQN实现</h2><p><b>3.2.1 编写基本DQN类的结构</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>class DQN():\n  # DQN Agent\n  def __init__(self, env): #初始化\n\n  def create_Q_network(self): #创建Q网络\n\n  def create_training_method(self): #创建训练方法\n\n  def perceive(self,state,action,reward,next_state,done): #感知存储信息\n\n  def train_Q_network(self): #训练网络\n\n  def egreedy_action(self,state): #输出带随机的动作\n\n  def action(self,state): #输出动作\n</code></pre></div><p>主要只需要以上几个函数。上面已经注释得很清楚，这里不再加以解释。</p><p>我们知道，我们的DQN一个很重要的功能就是要能存储数据，然后在训练的时候minibatch出来。所以，我们需要构造一个存储机制。这里使用deque来实现。</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>self.replay_buffer = deque()\n</code></pre></div><p><b>3.2.2 初始化</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>  def __init__(self, env):\n    # init experience replay\n    self.replay_buffer = deque()\n    # init some parameters\n    self.time_step = 0\n    self.epsilon = INITIAL_EPSILON\n    self.state_dim = env.observation_space.shape[0]\n    self.action_dim = env.action_space.n\n\n    self.create_Q_network()\n    self.create_training_method()\n\n    # Init session\n    self.session = tf.InteractiveSession()\n    self.session.run(tf.initialize_all_variables())\n</code></pre></div><p>这里要注意一点就是egreedy的epsilon是不断变小的，也就是随机性不断变小。怎么理解呢？就是一开始需要更多的探索，所以动作偏随机，慢慢的我们需要动作能够有效，因此减少随机。</p><p><b>3.2.3 创建Q网络</b></p><p>我们这里创建最基本的MLP，中间层设置为20：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>  def create_Q_network(self):\n    # network weights\n    W1 = self.weight_variable([self.state_dim,20])\n    b1 = self.bias_variable([20])\n    W2 = self.weight_variable([20,self.action_dim])\n    b2 = self.bias_variable([self.action_dim])\n    # input layer\n    self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n    # hidden layers\n    h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n    # Q Value layer\n    self.Q_value = tf.matmul(h_layer,W2) + b2\n\n  def weight_variable(self,shape):\n    initial = tf.truncated_normal(shape)\n    return tf.Variable(initial)\n\n  def bias_variable(self,shape):\n    initial = tf.constant(0.01, shape = shape)\n    return tf.Variable(initial)\n</code></pre></div><p>只有一个隐层，然后使用relu非线性单元。相信对MLP有了解的知友看上面的代码很easy！要注意的是我们state 输入的格式，因为使用minibatch，所以格式是[None,state_dim]</p><p><b>3.2.4 编写perceive函数</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>  def perceive(self,state,action,reward,next_state,done):\n    one_hot_action = np.zeros(self.action_dim)\n    one_hot_action[action] = 1\n     self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n    if len(self.replay_buffer) &amp;gt; REPLAY_SIZE:\n      self.replay_buffer.popleft()\n\n    if len(self.replay_buffer) &amp;gt; BATCH_SIZE:\n      self.train_Q_network()\n</code></pre></div><p>这里需要注意的一点就是动作格式的转换。我们在神经网络中使用的是one hot key的形式，而在OpenAI Gym中则使用单值。什么意思呢？比如我们输出动作是1，那么对应的one hot形式就是[0,1]，如果输出动作是0，那么one hot 形式就是[1,0]。这样做的目的是为了之后更好的进行计算。</p><p>在perceive中一个最主要的事情就是存储。然后根据情况进行train。这里我们要求只要存储的数据大于Batch的大小就开始训练。</p><p><b>3.2.5 编写action输出函数</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>def egreedy_action(self,state):\n    Q_value = self.Q_value.eval(feed_dict = {\n      self.state_input:[state]\n      })[0]\n    if random.random() &amp;lt;= self.epsilon:\n      return random.randint(0,self.action_dim - 1)\n    else:\n      return np.argmax(Q_value)\n\n    self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n\ndef action(self,state):\n    return np.argmax(self.Q_value.eval(feed_dict = {\n      self.state_input:[state]\n      })[0])\n</code></pre></div><p>区别之前已经说过，一个是根据情况输出随机动作，一个是根据神经网络输出。由于神经网络输出的是每一个动作的Q值，因此我们选择最大的那个Q值对应的动作输出。</p><p><b>3.2.6 编写training method函数</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>  def create_training_method(self):\n    self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n    self.y_input = tf.placeholder(\"float\",[None])\n    Q_action = tf.reduce_sum(tf.mul(self.Q_value,self.action_input),reduction_indices = 1)\n    self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n    self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n</code></pre></div><p>这里的y_input就是target Q值。我们这里采用Adam优化器，其实随便选择一个必然SGD，RMSProp都是可以的。可能比较不好理解的就是Q值的计算。这里大家记住动作输入是one hot key的形式，因此将Q_value和action_input向量相乘得到的就是这个动作对应的Q_value。然后用reduce_sum将数据维度压成一维。</p><p><b>3.2.7 编写training函数</b></p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>def train_Q_network(self):\n    self.time_step += 1\n    # Step 1: obtain random minibatch from replay memory\n    minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n    state_batch = [data[0] for data in minibatch]\n    action_batch = [data[1] for data in minibatch]\n    reward_batch = [data[2] for data in minibatch]\n    next_state_batch = [data[3] for data in minibatch]\n\n    # Step 2: calculate y\n    y_batch = []\n    Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n    for i in range(0,BATCH_SIZE):\n      done = minibatch[i][4]\n      if done:\n        y_batch.append(reward_batch[i])\n      else :\n        y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n\n    self.optimizer.run(feed_dict={\n      self.y_input:y_batch,\n      self.action_input:action_batch,\n      self.state_input:state_batch\n      })\n</code></pre></div><p>首先就是进行minibatch的工作，然后根据batch计算y_batch。最后就是用optimizer进行优化。</p><h2>4 整个程序</h2><p>以上便是编写DQN的全过程了。是不是很简单呢，下面再把整个程序放出如下：</p><div class=\"highlight\"><pre><code class=\"language-text\"><span></span>import gym\nimport tensorflow as tf\nimport numpy as np\nimport random\nfrom collections import deque\n\n# Hyper Parameters for DQN\nGAMMA = 0.9 # discount factor for target Q\nINITIAL_EPSILON = 0.5 # starting value of epsilon\nFINAL_EPSILON = 0.01 # final value of epsilon\nREPLAY_SIZE = 10000 # experience replay buffer size\nBATCH_SIZE = 32 # size of minibatch\n\nclass DQN():\n  # DQN Agent\n  def __init__(self, env):\n    # init experience replay\n    self.replay_buffer = deque()\n    # init some parameters\n    self.time_step = 0\n    self.epsilon = INITIAL_EPSILON\n    self.state_dim = env.observation_space.shape[0]\n    self.action_dim = env.action_space.n\n\n    self.create_Q_network()\n    self.create_training_method()\n\n    # Init session\n    self.session = tf.InteractiveSession()\n    self.session.run(tf.initialize_all_variables())\n\n  def create_Q_network(self):\n    # network weights\n    W1 = self.weight_variable([self.state_dim,20])\n    b1 = self.bias_variable([20])\n    W2 = self.weight_variable([20,self.action_dim])\n    b2 = self.bias_variable([self.action_dim])\n    # input layer\n    self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n    # hidden layers\n    h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n    # Q Value layer\n    self.Q_value = tf.matmul(h_layer,W2) + b2\n\n  def create_training_method(self):\n    self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n    self.y_input = tf.placeholder(\"float\",[None])\n    Q_action = tf.reduce_sum(tf.mul(self.Q_value,self.action_input),reduction_indices = 1)\n    self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n    self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n\n  def perceive(self,state,action,reward,next_state,done):\n    one_hot_action = np.zeros(self.action_dim)\n    one_hot_action[action] = 1\n    self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n    if len(self.replay_buffer) &amp;gt; REPLAY_SIZE:\n      self.replay_buffer.popleft()\n\n    if len(self.replay_buffer) &amp;gt; BATCH_SIZE:\n      self.train_Q_network()\n\n  def train_Q_network(self):\n    self.time_step += 1\n    # Step 1: obtain random minibatch from replay memory\n    minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n    state_batch = [data[0] for data in minibatch]\n    action_batch = [data[1] for data in minibatch]\n    reward_batch = [data[2] for data in minibatch]\n    next_state_batch = [data[3] for data in minibatch]\n\n    # Step 2: calculate y\n    y_batch = []\n    Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n    for i in range(0,BATCH_SIZE):\n      done = minibatch[i][4]\n      if done:\n        y_batch.append(reward_batch[i])\n      else :\n        y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n\n    self.optimizer.run(feed_dict={\n      self.y_input:y_batch,\n      self.action_input:action_batch,\n      self.state_input:state_batch\n      })\n\n  def egreedy_action(self,state):\n    Q_value = self.Q_value.eval(feed_dict = {\n      self.state_input:[state]\n      })[0]\n    if random.random() &amp;lt;= self.epsilon:\n      return random.randint(0,self.action_dim - 1)\n    else:\n      return np.argmax(Q_value)\n\n    self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n\n  def action(self,state):\n    return np.argmax(self.Q_value.eval(feed_dict = {\n      self.state_input:[state]\n      })[0])\n\n  def weight_variable(self,shape):\n    initial = tf.truncated_normal(shape)\n    return tf.Variable(initial)\n\n  def bias_variable(self,shape):\n    initial = tf.constant(0.01, shape = shape)\n    return tf.Variable(initial)\n# ---------------------------------------------------------\n# Hyper Parameters\nENV_NAME = 'CartPole-v0'\nEPISODE = 10000 # Episode limitation\nSTEP = 300 # Step limitation in an episode\nTEST = 10 # The number of experiment test every 100 episode\n\ndef main():\n  # initialize OpenAI Gym env and dqn agent\n  env = gym.make(ENV_NAME)\n  agent = DQN(env)\n\n  for episode in xrange(EPISODE):\n    # initialize task\n    state = env.reset()\n    # Train\n    for step in xrange(STEP):\n      action = agent.egreedy_action(state) # e-greedy action for train\n      next_state,reward,done,_ = env.step(action)\n      # Define reward for agent\n      reward_agent = -1 if done else 0.1\n      agent.perceive(state,action,reward,next_state,done)\n      state = next_state\n      if done:\n        break\n    # Test every 100 episodes\n    if episode % 100 == 0:\n      total_reward = 0\n      for i in xrange(TEST):\n        state = env.reset()\n        for j in xrange(STEP):\n          env.render()\n          action = agent.action(state) # direct action for test\n          state,reward,done,_ = env.step(action)\n          total_reward += reward\n          if done:\n            break\n      ave_reward = total_reward/TEST\n      print 'episode: ',episode,'Evaluation Average Reward:',ave_reward\n      if ave_reward &amp;gt;= 200:\n        break\n\nif __name__ == '__main__':\n  main()\n</code></pre></div><p>上面的代码就153行，我在github上加了网络的存储以及训练曲线的显示，代码200行左右！</p><h2>5 小结</h2><p>分析代码不是一件容易的事，这里我主要就是介绍编写的流程。具体代码还需要大家去理解吧！相信大家如果看懂了这150行代码，也就很清楚的知道DQN是怎么回事了。谢谢大家！</p>","updated":"2016-07-03T07:49:41.000Z","canComment":true,"commentPermission":"anyone","commentCount":93,"collapsedCount":0,"likeCount":216,"state":"published","isLiked":false,"slug":"21477488","lastestTipjarors":[{"isFollowed":false,"name":"风帆","headline":"","avatarUrl":"https://pic3.zhimg.com/v2-374c7bccbd2cd8df758a2fae59b55792_s.jpg","isFollowing":false,"type":"people","slug":"feng-fan-22-11","profileUrl":"https://www.zhihu.com/people/feng-fan-22-11","bio":"神经网络爱好者","hash":"a43a73c03a3aba962b3144adbbcd8037","uid":776426586625839100,"isOrg":false,"description":"","isOrgWhiteList":false,"avatar":{"id":"v2-374c7bccbd2cd8df758a2fae59b55792","template":"https://pic3.zhimg.com/{id}_{size}.jpg"}},{"isFollowed":false,"name":"紫杉","headline":"在每个领域乱晃的家伙，计算机，神经心理学，语言学，数学。Github博客：http://anie.me/","avatarUrl":"https://pic4.zhimg.com/20037ab07_s.jpg","isFollowing":false,"type":"people","slug":"zi-shan-43","profileUrl":"https://www.zhihu.com/people/zi-shan-43","bio":"","hash":"d56d775bba37cc18bfdea8ec67ab91cb","uid":50445737263104,"isOrg":false,"description":"在每个领域乱晃的家伙，计算机，神经心理学，语言学，数学。Github博客：http://anie.me/","isOrgWhiteList":false,"avatar":{"id":"20037ab07","template":"https://pic4.zhimg.com/{id}_{size}.jpg"}},{"isFollowed":false,"avatarUrl":"","isFollowing":false,"type":"people","slug":"0"},{"isFollowed":false,"name":"Zhang Andi","headline":"学生，弱鸡","avatarUrl":"https://pic2.zhimg.com/51cb041ec65f1c0f068a79586eda8591_s.jpg","isFollowing":false,"type":"people","slug":"zhang-andi","profileUrl":"https://www.zhihu.com/people/zhang-andi","bio":"给 Alexa 搬砖","hash":"e12f75f471a5c2ed335825df84e08710","uid":64344314347520,"isOrg":false,"description":"学生，弱鸡","isOrgWhiteList":false,"avatar":{"id":"51cb041ec65f1c0f068a79586eda8591","template":"https://pic2.zhimg.com/{id}_{size}.jpg"}},{"isFollowed":false,"name":"纳米垚","headline":"","avatarUrl":"https://pic1.zhimg.com/49c1e14e382d112e28f3beac3e79dbec_s.jpg","isFollowing":false,"type":"people","slug":"li-miao-80-71","profileUrl":"https://www.zhihu.com/people/li-miao-80-71","bio":"哈哈哈","hash":"a3d091684ccdff65583a3d6464878953","uid":63700568375296,"isOrg":false,"description":"","isOrgWhiteList":false,"avatar":{"id":"49c1e14e382d112e28f3beac3e79dbec","template":"https://pic1.zhimg.com/{id}_{size}.jpg"}}],"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic2.zhimg.com/178c11fcc99098e7fc039c8f7a96576d_r.png","links":{"comments":"/api/posts/21477488/comments"},"reviewers":[],"topics":[{"url":"https://www.zhihu.com/topic/20039099","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https://www.zhihu.com/topic/19813032","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https://www.zhihu.com/topic/19559450","id":"19559450","name":"机器学习"}],"adminClosedComment":false,"titleImageSize":{"width":1324,"height":368},"href":"/api/posts/21477488","excerptTitle":"","column":{"slug":"intelligentunit","name":"智能单元"},"tipjarState":"activated","tipjarTagLine":"您的赞赏是我们分享知识的动力！","sourceUrl":"","pageCommentsCount":93,"tipjarorCount":12,"annotationAction":[],"snapshotUrl":"","publishedTime":"2016-07-03T15:49:41+08:00","url":"/p/21477488","lastestLikers":[{"profileUrl":"https://www.zhihu.com/people/san-zhang-18-24","bio":"","hash":"752258bf07f8226fa6c886e85c903566","uid":36407749902336,"isOrg":false,"description":"","isOrgWhiteList":false,"slug":"san-zhang-18-24","avatar":{"id":"d10e36d32","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"name":"san zhang"},{"profileUrl":"https://www.zhihu.com/people/zhi-hu-38-10-82","bio":"汽车&amp;amp;计算机","hash":"12bb75c221f7a0ffaaea9d2e94aa3aa9","uid":697551180540166100,"isOrg":false,"description":"","isOrgWhiteList":false,"slug":"zhi-hu-38-10-82","avatar":{"id":"da8e974dc","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"name":"智胡"},{"profileUrl":"https://www.zhihu.com/people/coneco","bio":"计算机系","hash":"0719d1129a2f4c48acf8f9f0a4e837c0","uid":764445453814108200,"isOrg":false,"description":"","isOrgWhiteList":false,"slug":"coneco","avatar":{"id":"da8e974dc","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"name":"coneco"},{"profileUrl":"https://www.zhihu.com/people/zhangyl-60","bio":"有志事竟成","hash":"75674a641c5618e10d12eaa0a9ca9880","uid":749340115649712100,"isOrg":false,"description":"PLA.UST","isOrgWhiteList":false,"slug":"zhangyl-60","avatar":{"id":"b9483eb1f310b2976dffe88521a0cd41","template":"https://pic2.zhimg.com/{id}_{size}.jpg"},"name":"zhangyl"},{"profileUrl":"https://www.zhihu.com/people/lichuno","bio":"电磁场转cs的本科小渣渣。。。","hash":"e00d90bfe5b045721c704a6cbf855a2c","uid":632229424205140000,"isOrg":false,"description":"blog.lcyown.cn","isOrgWhiteList":false,"slug":"lichuno","avatar":{"id":"v2-1ef0448695039643f28387dc521f1851","template":"https://pic2.zhimg.com/{id}_{size}.jpg"},"name":"李春洋"}],"summary":"1 前言终于到了DQN系列真正的实战了。今天我们将一步一步的告诉大家如何用最短的代码实现基本的DQN算法，并且完成基本的RL任务。这恐怕也将是你在网上能找到的最详尽的DQN实战教程，当然了，代码也会是最短的。在本次实战中，我们不选择Atari游戏，而使用Op…","reviewingCommentsCount":0,"meta":{"previous":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic2.zhimg.com/6cfac1401554ece6dac25a7ef7f7fe79_r.jpg","links":{"comments":"/api/posts/21407711/comments"},"topics":[{"url":"https://www.zhihu.com/topic/19559450","id":"19559450","name":"机器学习"},{"url":"https://www.zhihu.com/topic/19551275","id":"19551275","name":"人工智能"},{"url":"https://www.zhihu.com/topic/19556664","id":"19556664","name":"科技"}],"adminClosedComment":false,"href":"/api/posts/21407711","excerptTitle":"","author":{"profileUrl":"https://www.zhihu.com/people/du-ke","bio":"CS231n资源在专栏文章中","hash":"928affb05b0b70a2c12e109d63b6bae5","uid":27591822016512,"isOrg":false,"description":"研究增强学习。积累学习方法论，实践健身训练体系。专栏：https://zhuanlan.zhihu.com/intelligentunit","isOrgWhiteList":false,"slug":"du-ke","avatar":{"id":"5ab5b93bd","template":"https://pic2.zhimg.com/{id}_{size}.jpg"},"name":"杜客"},"column":{"slug":"intelligentunit","name":"智能单元"},"content":"<p>译者注：本文<a href=\"https://zhuanlan.zhihu.com/intelligentunit\" class=\"internal\">智能单元</a>首发，译自斯坦福CS231n课程笔记<a href=\"http://link.zhihu.com/?target=http%3A//cs231n.github.io/optimization-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Backprop Note<i class=\"icon-external\"></i></a>，课程教师<a href=\"http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Andrej Karpathy<i class=\"icon-external\"></i></a>授权翻译。本篇教程由<a href=\"https://www.zhihu.com/people/du-ke\" class=\"internal\">杜客</a>翻译完成，<a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"internal\">堃堃</a>和<a href=\"https://www.zhihu.com/people/hmonkey\" class=\"internal\">巩子嘉</a>进行校对修改。译文含公式和代码，建议PC端阅读。</p><br><h2>原文如下：</h2><p>内容列表：</p><ul><li>简介</li><li>简单表达式和理解梯度</li><li>复合表达式，链式法则，反向传播</li><li>直观理解反向传播</li><li>模块：Sigmoid例子</li><li>反向传播实践：分段计算</li><li>回传流中的模式</li><li>用户向量化操作的梯度</li><li>小结</li></ul><h2>简介</h2><p><strong>目标</strong>：本节将帮助读者对<strong>反向传播</strong>形成直观而专业的理解。反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常<b>关键</b>。<br></p><p><strong>问题陈述</strong>：这节的核心问题是：给定函数<img src=\"http://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"> ，其中<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">是输入数据的向量，需要计算函数<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">关于<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">的梯度，也就是<img src=\"http://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29\" alt=\"\\nabla f(x)\" eeimg=\"1\">。</p><br><p><strong>目标</strong>：之所以关注上述问题，是因为在神经网络中<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">对应的是损失函数（<img src=\"http://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\">），输入<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">里面包含训练数据和神经网络的权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据<img src=\"http://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29%2Ci%3D1...N\" alt=\"(x_i,y_i),i=1...N\" eeimg=\"1\">、权重<img src=\"http://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\">和偏差<img src=\"http://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\">。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据<img src=\"http://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"> 上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如<img src=\"http://www.zhihu.com/equation?tex=W%2Cb\" alt=\"W,b\" eeimg=\"1\">）的梯度。然而<img src=\"http://www.zhihu.com/equation?tex=x_i%0A\" alt=\"x_i\n\" eeimg=\"1\"> 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。</p><br><p>如果读者之前对于利用链式法则计算偏微分已经很熟练，仍然建议浏览本篇笔记。因为它呈现了一个相对成熟的反向传播视角，在该视角中能看见基于实数值回路的反向传播过程，而对其细节的理解和收获将帮助读者更好地通过本课程。<br></p><h2>简单表达式和理解梯度</h2><p>从简单表达式入手可以为复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数<img src=\"http://www.zhihu.com/equation?tex=f%28x%2Cy%29%3Dxy\" alt=\"f(x,y)=xy\" eeimg=\"1\">。对两个输入变量分别求偏导数还是很简单的：<br></p><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3Dxy+%5Cto+%5Cfrac+%7Bdf%7D%7Bdx%7D%3Dy+%5Cquad+%5Cfrac+%7Bdf%7D%7Bdy%7D%3Dx\" alt=\"\\displaystyle f(x,y)=xy \\to \\frac {df}{dx}=y \\quad \\frac {df}{dy}=x\" eeimg=\"1\"><br><p><strong>解释</strong>：牢记这些导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。<br></p><img src=\"http://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D%3D+lim_%7Bh%5Cto+0%7D%5Cfrac%7Bf%28x%2Bh%29-f%28x%29%7D%7Bh%7D\" alt=\"\\frac{df(x)}{dx}= lim_{h\\to 0}\\frac{f(x+h)-f(x)}{h}\" eeimg=\"1\"><p>注意等号左边的分号和等号右边的分号不同，不是代表分数。相反，这个符号表示操作符<img src=\"http://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bdx%7D\" alt=\"\\frac{d}{dx}\" eeimg=\"1\">被应用于函数<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">，并返回一个不同的函数（导数）。对于上述公式，可以认为<img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\">值非常小，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。比如，若<img src=\"http://www.zhihu.com/equation?tex=x%3D4%2Cy%3D-3\" alt=\"x=4,y=-3\" eeimg=\"1\">，则<img src=\"http://www.zhihu.com/equation?tex=f%28x%2Cy%29%3D-12\" alt=\"f(x,y)=-12\" eeimg=\"1\">，<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">的导数<img src=\"http://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D-3\" alt=\"\\frac{\\partial f}{\\partial x}=-3\" eeimg=\"1\">。这就说明如果将变量<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">的值变大一点，整个表达式的值就会变小（原因在于负号），而且变小的量是<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">变大的量的三倍。通过重新排列公式可以看到这一点（<img src=\"http://www.zhihu.com/equation?tex=f%28x%2Bh%29%3Df%28x%29%2Bh+%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D\" alt=\"f(x+h)=f(x)+h \\frac{df(x)}{dx}\" eeimg=\"1\">）。同样，因为<img src=\"http://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+y%7D%3D4\" alt=\"\\frac{\\partial f}{\\partial y}=4\" eeimg=\"1\">，可以知道如果将<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">的值增加<img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\">，那么函数的输出也将增加（原因在于正号），且增加量是<img src=\"http://www.zhihu.com/equation?tex=4h\" alt=\"4h\" eeimg=\"1\">。<br></p><blockquote>函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。<br></blockquote><p>如上所述，梯度<img src=\"http://www.zhihu.com/equation?tex=%5Cnabla+f\" alt=\"\\nabla f\" eeimg=\"1\">是偏导数的向量，所以有<img src=\"http://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29%3D%5B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%2C%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+y%7D%5D%3D%5By%2Cx%5D\" alt=\"\\nabla f(x)=[\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}]=[y,x]\" eeimg=\"1\">。即使是梯度实际上是一个向量，仍然通常使用类似“<i>x上的梯度</i>”的术语，而不是使用如“<i>x的偏导数</i>”的正确说法，原因是因为前者说起来简单。</p><p>我们也可以对加法操作求导：<br></p><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3Dx%2By+%5Cto+%5Cfrac+%7Bdf%7D%7Bdx%7D%3D1%5Cquad%5Cfrac+%7Bdf%7D%7Bdy%7D%3D1\" alt=\"\\displaystyle f(x,y)=x+y \\to \\frac {df}{dx}=1\\quad\\frac {df}{dy}=1\" eeimg=\"1\"><p>这就是说，无论其值如何，<img src=\"http://www.zhihu.com/equation?tex=x%2Cy\" alt=\"x,y\" eeimg=\"1\">的导数均为1。这是有道理的，因为无论增加<img src=\"http://www.zhihu.com/equation?tex=x%2Cy\" alt=\"x,y\" eeimg=\"1\">中任一个的值，函数<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">的值都会增加，并且增加的变化率独立于<img src=\"http://www.zhihu.com/equation?tex=x%2Cy\" alt=\"x,y\" eeimg=\"1\">的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：<br><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3Dmax%28x%2Cy%29+%5Cto+%5Cfrac+%7Bdf%7D%7Bdx%7D%3D1+%28x%3E%3Dy%29+%5Cquad%5Cfrac+%7Bdf%7D%7Bdy%7D%3D1+%28y%3E%3Dx%29\" alt=\"\\displaystyle f(x,y)=max(x,y) \\to \\frac {df}{dx}=1 (x&amp;gt;=y) \\quad\\frac {df}{dy}=1 (y&amp;gt;=x)\" eeimg=\"1\"><br></p><p>上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若<img src=\"http://www.zhihu.com/equation?tex=x%3D4%2Cy%3D2\" alt=\"x=4,y=2\" eeimg=\"1\">，那么max是4，所以函数对于<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">就不敏感。也就是说，在<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">上增加<img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\">，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">增加一个很大的量，比如大于2，那么函数<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：<img src=\"http://www.zhihu.com/equation?tex=lim_%7Bh%5Cto+0%7D\" alt=\"lim_{h\\to 0}\" eeimg=\"1\">。<br></p><h2>使用链式法则计算复合表达式</h2><p>现在考虑更复杂的包含多个函数的复合函数，比如<img src=\"http://www.zhihu.com/equation?tex=f%28x%2Cy%2Cz%29%3D%28x%2By%29z\" alt=\"f(x,y,z)=(x+y)z\" eeimg=\"1\">。虽然这个表达足够简单，可以直接微分，但是在此使用一种有助于读者直观理解反向传播的方法。将公式分成两部分：<img src=\"http://www.zhihu.com/equation?tex=q%3Dx%2By\" alt=\"q=x+y\" eeimg=\"1\">和<img src=\"http://www.zhihu.com/equation?tex=f%3Dqz\" alt=\"f=qz\" eeimg=\"1\">。在前面已经介绍过如何对这分开的两个公式进行计算，因为<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">是<img src=\"http://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\">和<img src=\"http://www.zhihu.com/equation?tex=z\" alt=\"z\" eeimg=\"1\">相乘，所以<img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%3Dz%2C%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+z%7D%3Dq\" alt=\"\\displaystyle\\frac{\\partial f}{\\partial q}=z,\\frac{\\partial f}{\\partial z}=q\" eeimg=\"1\">，又因为<img src=\"http://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\">是<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">加<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">，所以<img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D%3D1%2C%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+y%7D%3D1\" alt=\"\\displaystyle\\frac{\\partial q}{\\partial x}=1,\\frac{\\partial q}{\\partial y}=1\" eeimg=\"1\">。然而，并不需要关心中间量<img src=\"http://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\">的梯度，因为<img src=\"http://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D\" alt=\"\\frac{\\partial f}{\\partial q}\" eeimg=\"1\">没有用。相反，函数<img src=\"http://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\">关于<img src=\"http://www.zhihu.com/equation?tex=x%2Cy%2Cz\" alt=\"x,y,z\" eeimg=\"1\">的梯度才是需要关注的。<b>链式法则</b>指出将这些梯度表达式链接起来的正确方式是相乘，比如<img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+x%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q%7D%5Cfrac%7B%5Cpartial+q%7D%7B%5Cpartial+x%7D\" alt=\"\\displaystyle\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\" eeimg=\"1\">。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下：</p><br><div class=\"highlight\"><pre><code class=\"language-python\"><span></span><span class=\"c1\"># 设置输入值</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">;</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span> <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">4</span>\n\n<span class=\"c1\"># 进行前向传播</span>\n<span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span> <span class=\"c1\"># q becomes 3</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">q</span> <span class=\"o\">*</span> <span class=\"n\">z</span> <span class=\"c1\"># f becomes -12</span>\n\n<span class=\"c1\"># 进行反向传播:</span>\n<span class=\"c1\"># 首先回传到 f = q * z</span>\n<span class=\"n\">dfdz</span> <span class=\"o\">=</span> <span class=\"n\">q</span> <span class=\"c1\"># df/dz = q, 所以关于z的梯度是3</span>\n<span class=\"n\">dfdq</span> <span class=\"o\">=</span> <span class=\"n\">z</span> <span class=\"c1\"># df/dq = z, 所以关于q的梯度是-4</span>\n<span class=\"c1\"># 现在回传到q = x + y</span>\n<span class=\"n\">dfdx</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">*</span> <span class=\"n\">dfdq</span> <span class=\"c1\"># dq/dx = 1. 这里的乘法是因为链式法则</span>\n<span class=\"n\">dfdy</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">*</span> <span class=\"n\">dfdq</span> <span class=\"c1\"># dq/dy = 1</span>\n</code></pre></div><p>最后得到变量的梯度<b>[dfdx, dfdy, dfdz]</b>，它们告诉我们函数<b>f</b>对于变量<b>[x, y, z]</b>的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写<b>df</b>了。这就是说，用<b>dq</b>来代替<b>dfdq</b>，且总是假设梯度是关于最终输出的。</p><p>这次计算可以被可视化为如下计算线路图像：<br></p><p>————————————————————————————————————————</p><p><img src=\"http://pic4.zhimg.com/213da7f66594510b45989bd134fc2d8b_b.jpg\" data-rawwidth=\"660\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"http://pic4.zhimg.com/213da7f66594510b45989bd134fc2d8b_r.jpg\">上图的真实值计算线路展示了计算的视觉化过程。<strong>前向传播</strong>从输入计算到输出（绿色），<strong>反向传播</strong>从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。</p><br><p>————————————————————————————————————————</p><h2>反向传播的直观理解</h2><p>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</p><blockquote>这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。<br></blockquote><p>下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以<b>x</b>和<b>y</b>的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果<b>x，y减小</b>（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。<br></p><p>因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。<br></p><h2>模块化：Sigmoid例子</h2><p>上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。现在看看一个表达式：<br></p><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28w%2Cx%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%28w_0x_0%2Bw_1x_1%2Bw_2%29%7D%7D\" alt=\"\\displaystyle f(w,x)=\\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}\" eeimg=\"1\"><br><p>在后面的课程中可以看到，这个表达式描述了一个含输入<b>x</b>和权重<b>w</b>的2维的神经元，该神经元使用了<i>sigmoid激活</i>函数。但是现在只是看做是一个简单的输入为x和w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：</p><br><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%29%3D%5Cfrac%7B1%7D%7Bx%7D+%5Cto+%5Cfrac%7Bdf%7D%7Bdx%7D%3D-1%2Fx%5E2\" alt=\"\\displaystyle f(x)=\\frac{1}{x} \\to \\frac{df}{dx}=-1/x^2\" eeimg=\"1\"><br><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f_c%28x%29%3Dc%2Bx+%5Cto+%5Cfrac%7Bdf%7D%7Bdx%7D%3D1\" alt=\"\\displaystyle f_c(x)=c+x \\to \\frac{df}{dx}=1\" eeimg=\"1\"><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%29%3De%5Ex+%5Cto+%5Cfrac%7Bdf%7D%7Bdx%7D%3De%5Ex\" alt=\"\\displaystyle f(x)=e^x \\to \\frac{df}{dx}=e^x\" eeimg=\"1\"><br><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f_a%28x%29%3Dax+%5Cto+%5Cfrac%7Bdf%7D%7Bdx%7D%3Da\" alt=\"\\displaystyle f_a(x)=ax \\to \\frac{df}{dx}=a\" eeimg=\"1\"><br><p>其中，函数<img src=\"http://www.zhihu.com/equation?tex=f_c\" alt=\"f_c\" eeimg=\"1\">使用对输入值进行了常量<img src=\"http://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\">的平移，<img src=\"http://www.zhihu.com/equation?tex=f_a\" alt=\"f_a\" eeimg=\"1\">将输入值扩大了常量<img src=\"http://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\">倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量<img src=\"http://www.zhihu.com/equation?tex=c%2Ca\" alt=\"c,a\" eeimg=\"1\">的梯度。整个计算线路如下：</p><br><p>———————————————————————————————————————</p><img src=\"http://pic1.zhimg.com/0799b3d6e5e92245ee937db3c26d1b80_b.png\" data-rawwidth=\"1490\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"1490\" data-original=\"http://pic1.zhimg.com/0799b3d6e5e92245ee937db3c26d1b80_r.png\"><p>使用sigmoid激活函数的2维神经元的例子。输入是[x0, x1]，可学习的权重是[w0, w1, w2]。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被sigmoid函数挤压到0到1之间。<br></p><p>————————————————————————————————————————</p><p>在上面的例子中可以看见一个函数操作的长链条，链条上的门都对<b>w</b>和<b>x</b>的点积结果进行操作。该函数被称为sigmoid函数<img src=\"http://www.zhihu.com/equation?tex=%5Csigma+%28x%29\" alt=\"\\sigma (x)\" eeimg=\"1\">。sigmoid函数关于其输入的求导是可以简化的(使用了在分子上先加后减1的技巧)：</p><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D\" alt=\"\\displaystyle\\sigma(x)=\\frac{1}{1+e^{-x}}\" eeimg=\"1\"><br><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cto%5Cfrac%7Bd%5Csigma%28x%29%7D%7Bdx%7D%3D%5Cfrac%7Be%5E%7B-x%7D%7D%7B%281%2Be%5E%7B-x%7D%29%5E2%7D%3D%28%5Cfrac%7B1%2Be%5E%7B-x%7D-1%7D%7B1%2Be%5E%7B-x%7D%7D%29%28%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D%29%3D%281-%5Csigma%28x%29%29%5Csigma%28x%29\" alt=\"\\displaystyle\\to\\frac{d\\sigma(x)}{dx}=\\frac{e^{-x}}{(1+e^{-x})^2}=(\\frac{1+e^{-x}-1}{1+e^{-x}})(\\frac{1}{1+e^{-x}})=(1-\\sigma(x))\\sigma(x)\" eeimg=\"1\"><br><p>可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下：<br></p><div class=\"highlight\"><pre><code class=\"language-python\"><span></span><span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"p\">]</span> <span class=\"c1\"># 假设一些随机数据和权重</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># 前向传播</span>\n<span class=\"n\">dot</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">dot</span><span class=\"p\">))</span> <span class=\"c1\"># sigmoid函数</span>\n\n<span class=\"c1\"># 对神经元反向传播</span>\n<span class=\"n\">ddot</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">f</span> <span class=\"c1\"># 点积变量的梯度, 使用sigmoid函数求导</span>\n<span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ddot</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ddot</span><span class=\"p\">]</span> <span class=\"c1\"># 回传到x</span>\n<span class=\"n\">dw</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ddot</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">ddot</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span> <span class=\"o\">*</span> <span class=\"n\">ddot</span><span class=\"p\">]</span> <span class=\"c1\"># 回传到w</span>\n<span class=\"c1\"># 完成！得到输入的梯度</span>\n</code></pre></div><p><strong>实现提示：分段反向传播</strong>。上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量<b>dot</b>，它装着<b>w</b>和<b>x</b>的点乘结果。在反向传播的时，就可以（反向地）计算出装着<b>w</b>和<b>x</b>等的梯度的对应的变量（比如<b>ddot</b>，<b>dx</b>和<b>dw</b>）。</p><br><p>本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以“链”在一起，让代码量更少，效率更高。<br></p><h2>反向传播实践：分段计算</h2><p>看另一个例子。假设有如下函数：<br></p><img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+f%28x%2Cy%29%3D%5Cfrac%7Bx%2B%5Csigma%28y%29%7D%7B%5Csigma%28x%29%2B%28x%2By%29%5E2%7D\" alt=\"\\displaystyle f(x,y)=\\frac{x+\\sigma(y)}{\\sigma(x)+(x+y)^2}\" eeimg=\"1\"><br><p>首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对<img src=\"http://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\">或<img src=\"http://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\">进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：<br></p><div class=\"highlight\"><pre><code class=\"language-python\"><span></span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"mi\">3</span> <span class=\"c1\"># 例子数值</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">4</span>\n\n<span class=\"c1\"># 前向传播</span>\n<span class=\"n\">sigy</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">y</span><span class=\"p\">))</span> <span class=\"c1\"># 分子中的sigmoi          #(1)</span>\n<span class=\"n\">num</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">sigy</span> <span class=\"c1\"># 分子                                    #(2)</span>\n<span class=\"n\">sigx</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">))</span> <span class=\"c1\"># 分母中的sigmoid         #(3)</span>\n<span class=\"n\">xpy</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span>                                              <span class=\"c1\">#(4)</span>\n<span class=\"n\">xpysqr</span> <span class=\"o\">=</span> <span class=\"n\">xpy</span><span class=\"o\">**</span><span class=\"mi\">2</span>                                          <span class=\"c1\">#(5)</span>\n<span class=\"n\">den</span> <span class=\"o\">=</span> <span class=\"n\">sigx</span> <span class=\"o\">+</span> <span class=\"n\">xpysqr</span> <span class=\"c1\"># 分母                                #(6)</span>\n<span class=\"n\">invden</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"n\">den</span>                                       <span class=\"c1\">#(7)</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">num</span> <span class=\"o\">*</span> <span class=\"n\">invden</span> <span class=\"c1\"># 搞定！                                 #(8)</span>\n</code></pre></div><p>┗|｀O′|┛ 嗷~~，到了表达式的最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(<strong>sigy, num, sigx, xpy, xpysqr, den, invden</strong>)进行回传。我们会有同样数量的变量，但是都以<strong>d</strong>开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。</p><br><div class=\"highlight\"><pre><code class=\"language-python\"><span></span><span class=\"c1\"># 回传 f = num * invden</span>\n<span class=\"n\">dnum</span> <span class=\"o\">=</span> <span class=\"n\">invden</span> <span class=\"c1\"># 分子的梯度                                         #(8)</span>\n<span class=\"n\">dinvden</span> <span class=\"o\">=</span> <span class=\"n\">num</span>                                                     <span class=\"c1\">#(8)</span>\n<span class=\"c1\"># 回传 invden = 1.0 / den </span>\n<span class=\"n\">dden</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">den</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"n\">dinvden</span>                                <span class=\"c1\">#(7)</span>\n<span class=\"c1\"># 回传 den = sigx + xpysqr</span>\n<span class=\"n\">dsigx</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dden</span>                                                <span class=\"c1\">#(6)</span>\n<span class=\"n\">dxpysqr</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dden</span>                                              <span class=\"c1\">#(6)</span>\n<span class=\"c1\"># 回传 xpysqr = xpy**2</span>\n<span class=\"n\">dxpy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">xpy</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dxpysqr</span>                                        <span class=\"c1\">#(5)</span>\n<span class=\"c1\"># 回传 xpy = x + y</span>\n<span class=\"n\">dx</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dxpy</span>                                                   <span class=\"c1\">#(4)</span>\n<span class=\"n\">dy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dxpy</span>                                                   <span class=\"c1\">#(4)</span>\n<span class=\"c1\"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span>\n<span class=\"n\">dx</span> <span class=\"o\">+=</span> <span class=\"p\">((</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">sigx</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">sigx</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dsigx</span> <span class=\"c1\"># Notice += !! See notes below  #(3)</span>\n<span class=\"c1\"># 回传 num = x + sigy</span>\n<span class=\"n\">dx</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dnum</span>                                                  <span class=\"c1\">#(2)</span>\n<span class=\"n\">dsigy</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dnum</span>                                                <span class=\"c1\">#(2)</span>\n<span class=\"c1\"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span>\n<span class=\"n\">dy</span> <span class=\"o\">+=</span> <span class=\"p\">((</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">sigy</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">sigy</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">dsigy</span>                                 <span class=\"c1\">#(1)</span>\n<span class=\"c1\"># 完成! 嗷~~</span>\n</code></pre></div><p>需要注意的一些东西：</p><p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p><p><strong>在不同分支的梯度要相加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用<b>+=</b>而不是<b>=</b>来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的<i>多元链式法则</i>，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。</p><br><h2>回传流中的模式</h2><p>一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观地解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：<br></p><p>——————————————————————————————————————————</p><p><img src=\"http://pic2.zhimg.com/39162d0c528144362cc09f1965d710d1_b.jpg\" data-rawwidth=\"660\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"http://pic2.zhimg.com/39162d0c528144362cc09f1965d710d1_r.jpg\">一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。<br></p><p>——————————————————————————————————————————</p><p>从上例可知：</p><p><strong>加法门单元</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。</p><p><strong>取最大值门单元</strong>对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了<b>z</b>变量，因为<b>z</b>的值比<b>w</b>高，于是<b>w</b>的梯度保持为0。</p><p><strong>乘法门单元</strong>相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，<b>x</b>的梯度是-4.00x2.00=-8.00。</p><p><i>非直观影响及其结果</i>。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积<img src=\"http://www.zhihu.com/equation?tex=w%5ETx_i\" alt=\"w^Tx_i\" eeimg=\"1\">，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本<img src=\"http://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\">乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。</p><h2>用向量化操作计算梯度</h2><p>上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。</p><p><strong>矩阵相乘的梯度</strong>：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：<br></p><div class=\"highlight\"><pre><code class=\"language-python\"><span></span><span class=\"c1\"># 前向传播</span>\n<span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 假设我们得到了D的梯度</span>\n<span class=\"n\">dD</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">D</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># 和D一样的尺寸</span>\n<span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">dD</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"c1\">#.T就是对矩阵进行转置</span>\n<span class=\"n\">dX</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dD</span><span class=\"p\">)</span>\n</code></pre></div><p><i>提示：要分析维度！</i>注意不需要去记忆<b>dW</b>和<b>dX</b>的表达，因为它们很容易通过维度推导出来。例如，权重的梯度dW的尺寸肯定和权重矩阵W的尺寸是一样的，而这又是由<b>X</b>和<b>dD</b>的矩阵乘法决定的（在上面的例子中<b>X</b>和<b>W</b>都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，<b>X</b>的尺寸是[10x3]，<b>dD</b>的尺寸是[5x3]，如果你想要dW和W的尺寸是[5x10]，那就要<b>dD.dot(X.T)</b>。</p><p><strong>使用小而具体的例子</strong>：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。<br></p><h2>小结</h2><ul><li><p>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。</p></li><li><p>讨论了<strong>分段计算</strong>在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。</p></li></ul><p>在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。</p><h2>参考文献</h2><ul><li><a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.05767\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Automatic differentiation in machine learning: a survey<i class=\"icon-external\"></i></a></li></ul><p><b>反向传播笔记全文翻译完</b>。</p><h2>译者反馈</h2><ol><li>转载须全文转载注明原文链接，否则保留维权权利；<br></li><li><b>知乎上关于反向传播的问题很多，希望知友们能点赞帮助本文广泛传播为更多人解惑</b>；</li><li>请知友们通过评论和私信等方式批评指正，贡献者均会补充提及；</li><li>感谢知友<a class=\"internal\" href=\"https://www.zhihu.com/people/roachsinai\">roach sinai</a>、<a href=\"https://www.zhihu.com/people/zheng-shen-hai\" class=\"internal\">郑申海</a>和<a href=\"https://www.zhihu.com/people/chen-yi-91-27\" class=\"internal\">陈一</a>的细节修改建议。</li></ol>","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":21407711,"publishedTime":"2016-06-29T19:41:06+08:00","url":"/p/21407711","title":"CS231n课程笔记翻译：反向传播笔记","summary":"译者注：本文<a href=\"https://zhuanlan.zhihu.com/intelligentunit\" data-editable=\"true\" data-title=\"智能单元\" class=\"\">智能单元</a>首发，译自斯坦福CS231n课程笔记<a href=\"http://cs231n.github.io/optimization-2/\" class=\"\" data-editable=\"true\" data-title=\"Optimization Note\">Backprop Note</a>，课程教师<a href=\"https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\"\" data-editable=\"true\" data-title=\"Andrej Karpathy\">Andrej Karpathy</a>授权翻译。本篇教程由<a href=\"https://www.zhihu.com/people/du-ke\" class=\"\" data-editable=\"true\" data-title=\"杜客\">杜客</a>翻译完成，<a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"\" data-editable=\"true\" data-title=\"堃堃\">堃堃</a>和<a href=\"https://www.zhihu.com/people/hmonkey\" class=\"\" data-editable=\"true\" data-title=\"巩子嘉\">巩子嘉</a>进行校对修改。译文含公式和代码，建议PC端阅读。 原文如下：内容列表：简介简单表达式和理解梯度复合表达式，…","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":0,"likesCount":0},"next":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic2.zhimg.com/6f589df38509d14f839737645322a011_r.jpg","links":{"comments":"/api/posts/21470871/comments"},"topics":[{"url":"https://www.zhihu.com/topic/19556664","id":"19556664","name":"科技"},{"url":"https://www.zhihu.com/topic/19551273","id":"19551273","name":"机器人"},{"url":"https://www.zhihu.com/topic/19813032","id":"19813032","name":"深度学习（Deep Learning）"}],"adminClosedComment":false,"href":"/api/posts/21470871","excerptTitle":"","author":{"profileUrl":"https://www.zhihu.com/people/flood-sung","bio":"AGI Independent Researcher","hash":"23deec836a24f295500a6d740011359c","uid":654375804428095500,"isOrg":false,"description":"本人已委托“维权骑士”（rightknights.com)为我的文章进行维权行动，如需转载前往https://rightknights.com/material/author?id=2960 获取合法授权。联系本人有问题直接问，除了技术问题之外，其余一律不答！谢谢！","isOrgWhiteList":false,"slug":"flood-sung","avatar":{"id":"73a71f47d66e280735a6c786131bdfe2","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"name":"Flood Sung"},"column":{"slug":"intelligentunit","name":"智能单元"},"content":"<h2>1 机器人革命？Are you joking？</h2><p>如果问我Google旗下的DeepMind会是一家怎样的公司的话，我的回答会是</p><p>“DeepMind将可能成为一家世界上最牛逼的机器人公司！”</p><p><img src=\"https://pic4.zhimg.com/4b2176ca2c4ed4238a17873f12d337bf_b.png\" data-rawwidth=\"1600\" data-rawheight=\"780\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/4b2176ca2c4ed4238a17873f12d337bf_r.png\">因为，人工智能的最佳出口就是机器人！并且机器人是人工智能的最直接体现。上图截自DeepMind CEO <a href=\"http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DJ6Yb5AlHZDw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Demis Hassabis的演讲<i class=\"icon-external\"></i></a>! DeepMind选择的方式就是从简单的做起，从Atari游戏做起，从围棋做起，然后一步一步的伸向机器人！从仿真的机器人到真实世界的机器人！DeepMind的目标是什么？Solve Intelligence！解决智能。而他们确实真的走在解决智能的路上！</p><p>现在，DeepMind最新的成果<a href=\"http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.04671\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Progressive Neural Networks<i class=\"icon-external\"></i></a>终于伸向真正的机器人了！</p><img src=\"https://pic3.zhimg.com/c8ba4484a58c0f6baba7da244525181e_b.png\" data-rawwidth=\"2124\" data-rawheight=\"966\" class=\"origin_image zh-lightbox-thumb\" width=\"2124\" data-original=\"https://pic3.zhimg.com/c8ba4484a58c0f6baba7da244525181e_r.png\"><p>做了个什么事情呢？就是<b>在仿真环境中训练一个机械臂移动，然后训练好之后，可以把知识迁移到真实的机械臂上，真实的机械臂稍加训练也可以做到和仿真一样的效果！</b></p><p>视频在这：<a href=\"http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DYZz5Io_ipi8\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">youtube.com/watch?</span><span class=\"invisible\">v=YZz5Io_ipi8</span><span class=\"ellipsis\"></span><i class=\"icon-external\"></i></a></p><p>这意味着什么呢？</p><p>如果我们能够训练一个机器人在仿真环境中干家务，那么移植到真实的机器人中，这个机器人经过一定训练也就可以干家务了！</p><p>那么仿真环境有什么好处？我们可以搞几十个几百个机器人同时训练，我们也不用担心机器人会损坏！这将大大加速机器人的学习过程！</p><p>所以今天这篇最前沿的标题比较大，我认为迁移深度增强学习将意味着机器人革命成为可能！机器人进入千家万户将不再只是想想，而是真正能够落地！</p><p>为什么这么认为？有以下几个原因。</p><ol><li><b>传统的机器人的方法是实现不了通用的机器人的！</b>因为传统的机器人控制方法基本都依赖于人工，而人工意味着局限性很大，我们不可能对每一种机器人面对的情况都编程。因此，要实现机器人革命，唯一的可能就是让机器人能够学习！通过学习来掌握完成任务的技巧！</li><li><b>深度增强学习是替代传统机器人控制方法的最有潜力方法！</b>深度增强学习能够使机器人实现端到端的学习！但是，大家都知道的是，深度学习需要大量的样本，大量的实验，这用在图像识别上可以，但用在真实的机器人上几乎是不现实的。一方面是增强学习依靠大量的试错。但机器人一旦试错就直接坏了怎么办？另一方面造几十个上百个机器人来训练需要太多的资金了，即使是土豪如Google恐怕也承受不起！ 所以，怎么办呢？两个出路：一个就是研究One-Shot Learning,也就是快速学习！如果机器人看一下人的演示就能很快学会动作，那OK。另一个出路就是迁移学习Transfer Learning！让机器人在仿真中学会，然后把知识迁移到真实的环境。大家可以想象，未来的虚拟场景肯定可以做到和真实的场景几乎一模一样！所以，只要能够迁移知识，那么在仿真中训练机器人是最佳途径！</li><li>那么现在，迁移深度增强学习迈出了第一步，而且成功了！我们就可以相信，接下来的发展将可以学习更复杂的任务，并且迁移到真实的机器人中完成！如果这一步实现了，那么，我们是不是完全可以看到真正的走近每个家庭的机器人已经不远了呢？</li></ol><p>所以有知友说迁移学习很重要不无道理！</p><p>从虚拟到现实，迁移深度增强学习让机器人革命不再遥远！</p><h2>2 迁移学习，比你想象的重要！</h2><p>在这里，我想说迁移学习之所以意义非凡在于一点，就是<br></p><blockquote>机器人迁移学习的成功意味着神经网络能够存储并提取“概念性”的东西！<br></blockquote><p>什么意思呢？</p><br><blockquote>计算机已经能够存储和提取抽象的知识！<br></blockquote><p>让计算机直接存储感知信息比较简单，比如一幅图像，一段音频。</p><p>但是，如何让计算机学会一件事情并且把“记忆”移植出来呢？</p><p>这就好比科比打球不是很厉害吗？要是<b>能把科比打球的记忆移植过来我们是不是就分分钟成为篮球高手了？</b></p><p>但这到底有多难呢？<b>这种问题恐怕几年前都没人敢想吧！计算机也可以做到吗？</b></p><p>我们还是以打球来说明！首先这个打球记忆到底是如何存储的呢？没人知道，这段记忆有经验（怎么打），有概念（篮球，篮筐。。。），有感知（当前场景），有控制（肌肉运动）。简直不能再复杂了！我们完全无法分析到底大脑是怎么存储这些有具象，也有抽象的记忆。接下来，更困难的问题：大脑的记忆那么多，又要如何才能把打球的记忆给提取出来呢？要能够定位大脑的这部分记忆，要能够知道记忆的格式，看是JPG还是PNG[呲牙]。。。，最后，还有能上传这部分记忆到新的大脑里！</p><p>上面说的移植记忆的问题简直超出了大部分人可以想象的范围了！</p><p>那么，计算机也可以吗？</p><p>计算机也可以？！</p><p>也可以！</p><p>可以！</p><p>仿真机械臂在仿真中训练了一个神经网络（“大脑”），接下来把神经网络连接到真实机械臂的神经网络（“大脑”），然后真实的机械臂在训练过程中，从仿真神经网络中提取了有用的信息（到底是什么不知道），然后大大加速和加强了真实机械臂的训练！</p><p>我又重述了一下DeepMind的成果，但现在的感觉是不是要Oh My God了！</p><p>这就是移植记忆的小Demo呀！</p><blockquote>在人类还没有搞清楚大脑的记忆是怎样的时候，人类却已经让计算机能够移植它的“记忆”了！<br></blockquote><p>反过来想，大脑的记忆，不也就是无数个神经元嘛！当计算机已经能够移植记忆的时候，我已经感觉到移植大脑记忆也不远了！</p><p>最后，这个工作的幕后老大是个美女！</p><img src=\"https://pic4.zhimg.com/4549843849c9809fde0db7eab573895f_b.png\" data-rawwidth=\"922\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic4.zhimg.com/4549843849c9809fde0db7eab573895f_r.png\"><p>引用她的一句话（注意我加黑的地方）：</p><blockquote> Deep learning is producing exciting new applications and advances at an amazing rate. But I'm more excited by what deep neural nets will eventually tell us about the fundamentals of human intelligence. <b>Fast learning, slow learning, memory, and imagination</b>: these are all areas that could be <b>cracked by deep nets in coming years</b>, and in doing so we may unlock some basic truths of how the brain works.</blockquote><p>最后的最后，还是那句话：</p><blockquote>这就是人工智能的最前沿！<br></blockquote><br><p>---------------------------------------------------------------------------------------------------------</p><p>下面进入专业科普时间！我们来看看到底计算机是怎么做到的！有兴趣也有基础的知友可以往下看！</p><h2>1 没想到是这么简单的idea！</h2><p><img src=\"https://pic3.zhimg.com/bed52fe0d53a3e798a7cc799b8d73036_b.png\" data-rawwidth=\"1582\" data-rawheight=\"808\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic3.zhimg.com/bed52fe0d53a3e798a7cc799b8d73036_r.png\">用几句话就能够说明白这个所谓的progressive neural networks到底是什么了！简直不能再简单！</p><p>就是：</p><p>Step 1：构造一个多层的神经网络，训练某一个任务，上图第一列</p><p>Step 2：构建第二个多层的神经网络，然后固定第一列也就是上一个任务的神经网络，将上一列的神经网络的每一层（注意是每一层）都通过a处理连接到第二列的神经网络的每一层作为额外输入。也就是第二个神经网络每一层除了原始的输入，还加上经过a处理的之前的神经网络对应层的输入。</p><p>Step 3：构建第三个多层神经网络，训练第三个任务，将前两列的神经网络固定，然后同上一样的方法连接到第三个神经网络中。</p><p>上图的线很清楚的表示了这个过程。</p><p>这就是把神经网络和神经网络连起来的方法！</p><p>a的作用其实主要是为了降维和输入的维度统一（与原始输入匹配），用简单的MLP来表示！</p><p>除此之外，增强学习算法没有任何变化。文章中使用A3C算法，一个比DQN强4倍的算法！</p><p>总的来说，就是抽取之前的神经网络的信息与当前的输入信息融合，然后训练！训练的效果就可以和没有加前面的神经网络的方法对比，如果效果好很多说明前面的神经网络有用，知识有迁移！</p><p>这种方法的好处就是之前的训练都保留，不至于像fine tune那样更改原来的网络！而且每一层的特征信息都能得到迁移，并且能够更好的具化分析。</p><p>缺点就是参数的数量会随着任务的增加而大量增加！并且不同任务的设计需要人工知识。</p><h2>2 实验</h2><p>文章做了三个实验，分别是Pong的变种实验，Atari实验以及Labyrinth实验。那么效果大部分都不错！这里就不贴了，主要还是来看一下那个机械臂的实验。这个实验在文章中并没有。</p><p><img src=\"https://pic1.zhimg.com/127b768f7c15e8979f607562aa3bc8e4_b.png\" data-rawwidth=\"1868\" data-rawheight=\"1072\" class=\"origin_image zh-lightbox-thumb\" width=\"1868\" data-original=\"https://pic1.zhimg.com/127b768f7c15e8979f607562aa3bc8e4_r.png\">这个仿真机械臂训练的是到达任务（Reacher task），也就是让机械臂的头能够达到某一个位置。</p><p>注意这里是输入是图像哦，而且只有图像，连机械臂的关节信息都没有，输出就是各个关节的速度。采用的网络就是卷积神经网络+LSTM+Softmax，使用A3C算法。这里我们可以看到用了16个线程threads来训练，约等于同时开了16台机械臂。</p><p>接下来，到真实的环境</p><p><img src=\"https://pic1.zhimg.com/f3dbfc72fb03c92d3c826746e107c0bc_b.png\" data-rawwidth=\"1806\" data-rawheight=\"926\" class=\"origin_image zh-lightbox-thumb\" width=\"1806\" data-original=\"https://pic1.zhimg.com/f3dbfc72fb03c92d3c826746e107c0bc_r.png\">这个真实的机械臂训练的任务和仿真的一样，但是不同的是输入不一样。这里我不清楚proprioception指的是什么，但从后面的网络结构MLP可以看出绝对不是图像数据，可能是机械臂的关节数据。也就是真实的机械臂的输入和仿真的不一样。这样也能迁移学习？真的是很诡异啊！但，事实是：</p><p><img src=\"https://pic4.zhimg.com/f0ae11d55ca57da2d7baa3a78e560ea7_b.png\" data-rawwidth=\"1070\" data-rawheight=\"750\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic4.zhimg.com/f0ae11d55ca57da2d7baa3a78e560ea7_r.png\">红色是使用了之前的仿真网络的，蓝色是没有的。效果显而易见。</p><p>接下来，在完成这个任务的基础之上，DeepMind又继续训练机械臂做“Catch”任务及“Catch the bee\"抓取任务，都取得了成功。进一步验证了Progressive Neural Networks的可行性。</p><h2>3 小结</h2><p>我更乐意称这个工作为迁移深度增强学习，以表示和以前迁移学习的不一样，主要在于迁移的知识不一样了。相信这个工作的继续发展将带来重大的突破。</p><br><p>图片引用：<a href=\"http://link.zhihu.com/?target=http%3A//juxi.net/workshop/deep-learning-rss-2016/slides/Raia_Hadsell_RSS_DL_workshop.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">juxi.net/workshop/deep-</span><span class=\"invisible\">learning-rss-2016/slides/Raia_Hadsell_RSS_DL_workshop.pdf</span><span class=\"ellipsis\"></span><i class=\"icon-external\"></i></a></p><h2>本文为原创文章，未经作者允许不得转载！</h2>","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":21470871,"publishedTime":"2016-07-04T12:52:09+08:00","url":"/p/21470871","title":"最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！","summary":"1 机器人革命？Are you joking？如果问我Google旗下的DeepMind会是一家怎样的公司的话，我的回答会是“DeepMind将可能成为一家世界上最牛逼的机器人公司！”因为，人工智能的最佳出口就是机器人！并且机器人是人工智能的最直接体现。上图截自DeepMind CEO <a href=\"http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DJ6Yb5AlHZDw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">D…<i class=\"icon-external\"></i></a>","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":0,"likesCount":0}},"annotationDetail":null,"commentsCount":93,"likesCount":216,"FULLINFO":true}},"User":{"ma-qi-xiang":{"isFollowed":false,"name":"马麒翔","headline":"","avatarUrl":"https://pic3.zhimg.com/v2-725346aa0ac14074ea554d9a2bee1412_s.jpg","isFollowing":false,"type":"people","slug":"ma-qi-xiang","bio":"东南大学硕士在读 足球爱好者 马德里竞技球迷","hash":"b915cacbb94bb0b58e2afad994f71a54","uid":46270169219072,"links":{"columns":"/api/me/columns"},"isOrg":false,"pendingColumns":[],"activated":true,"allowShareDaily":false,"isBindPhone":false,"mutedInfo":{"muted":false,"reason":null},"description":"","muted":false,"profileUrl":"https://www.zhihu.com/people/ma-qi-xiang","avatar":{"id":"v2-725346aa0ac14074ea554d9a2bee1412","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false,"email":"773954470@qq.com","columns":[]},"flood-sung":{"isFollowed":false,"name":"Flood Sung","headline":"本人已委托“维权骑士”（rightknights.com)为我的文章进行维权行动，如需转载前往https://rightknights.com/material/author?id=2960 获取合法授权。联系本人有问题直接问，除了技术问题之外，其余一律不答！谢谢！","avatarUrl":"https://pic3.zhimg.com/73a71f47d66e280735a6c786131bdfe2_s.jpg","isFollowing":false,"type":"people","slug":"flood-sung","bio":"AGI Independent Researcher","hash":"23deec836a24f295500a6d740011359c","uid":654375804428095500,"isOrg":false,"description":"本人已委托“维权骑士”（rightknights.com)为我的文章进行维权行动，如需转载前往https://rightknights.com/material/author?id=2960 获取合法授权。联系本人有问题直接问，除了技术问题之外，其余一律不答！谢谢！","profileUrl":"https://www.zhihu.com/people/flood-sung","avatar":{"id":"73a71f47d66e280735a6c786131bdfe2","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false,"badge":{"identity":null,"bestAnswerer":null}}},"Comment":{},"favlists":{}},"me":{"slug":"ma-qi-xiang"},"global":{},"columns":{"intelligentunit":{"following":true,"canManage":false,"href":"/api/columns/intelligentunit","name":"智能单元","creator":{"slug":"du-ke"},"url":"/intelligentunit","slug":"intelligentunit","avatar":{"id":"4a97d93d652f45ededf2ebab9a13f22b","template":"https://pic4.zhimg.com/{id}_{size}.jpeg"}}},"columnPosts":{},"postComments":{},"postReviewComments":{"comments":[],"newComments":[],"hasMore":true},"favlistsByUser":{},"favlistRelations":{},"promotions":{},"switches":{"couldAddVideo":false},"draft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null}},"config":{"userNotBindPhoneTipString":{}},"recommendPosts":{"articleRecommendations":[],"columnRecommendations":[]},"env":{"isAppView":false,"appViewConfig":{"content_padding_top":128,"content_padding_bottom":56,"content_padding_left":16,"content_padding_right":16,"title_font_size":22,"body_font_size":16,"is_dark_theme":false,"can_auto_load_image":true,"app_info":"OS=iOS"},"isApp":false},"sys":{}}</textarea>

    <script src="//static.zhihu.com/hemingway/common.f8ca20f515651bf5b853.js"></script>
<script src="//static.zhihu.com/hemingway/app.bb747dda89659475f01c.js"></script>
<script src="//static.zhihu.com/hemingway/raven.e117dbda2c5e9097b47f.js" async defer></script>
  </body>
</html>
